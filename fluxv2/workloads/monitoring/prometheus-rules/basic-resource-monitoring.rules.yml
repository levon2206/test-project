apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    app: kube-prometheus-stack
    release: prometheus-operator
  name: basic-resource-monitoring.rules
  namespace: monitoring
spec:
  groups:
  - name: prometheus-self-monitoring
    rules:
    - alert: Prometheus job missing
      annotations:
        message: A Prometheus job has disappeared
      expr: 'absent(up{job="prometheus-operator-kube-p-prometheus"})'
      labels:
        severity: warning
    - alert: Prometheus target missing
      annotations:
        message: A Prometheus target has disappeared. An exporter might be crashed.
      expr: 'up == 0'
      labels:
        severity: critical
    - alert: Prometheus all targets missing
      annotations:
        message: A Prometheus job does not have living target anymore.
      expr: 'count by (job) (up) == 0'
      labels:
        severity: critical
    - alert: Prometheus configuration reload failure
      annotations:
        message: Prometheus configuration reload error
      expr: 'prometheus_config_last_reload_successful != 1'
      labels:
        severity: warning
    - alert: Prometheus too many restarts
      annotations:
        message: Prometheus has restarted more than twice in the last 15 minutes.
          It might be crashlooping.
      expr: 'changes(process_start_time_seconds{job=~"prometheus|pushgateway|alertmanager"}[15m])
        > 2'
      labels:
        severity: warning
    - alert: Prometheus AlertManager configuration reload failure
      annotations:
        message: AlertManager configuration reload error
      expr: 'alertmanager_config_last_reload_successful != 1'
      labels:
        severity: warning
    - alert: Prometheus AlertManager config not synced
      annotations:
        message: Configurations of AlertManager cluster instances are out of sync
      expr: 'count(count_values("config_hash", alertmanager_config_hash)) > 1'
      labels:
        severity: warning
    - alert: Prometheus AlertManager E2E dead man switch
      annotations:
        message: Prometheus DeadManSwitch is an always-firing alert. It's used
          as an end-to-end test of Prometheus through the Alertmanager.
      expr: 'vector(1)'
      labels:
        severity: critical
    - alert: Prometheus not connected to alertmanager
      annotations:
        message: Prometheus cannot connect the alertmanager
      expr: "prometheus_notifications_alertmanagers_discovered < 1"
      labels:
        severity: critical
    - alert: Prometheus rule evaluation failures
      annotations:
        message: 'Prometheus encountered {{ $value }} rule evaluation failures,
          leading to potentially ignored alerts.'
      expr: 'increase(prometheus_rule_evaluation_failures_total[3m]) > 0'
      labels:
        severity: critical
    - alert: Prometheus template text expansion failures
      annotations:
        message: 'Prometheus encountered {{ $value }} template text expansion
          failures'
      expr: 'increase(prometheus_template_text_expansion_failures_total[3m]) >
        0'
      labels:
        severity: critical
    - alert: Prometheus rule evaluation slow
      annotations:
        message: 'Prometheus rule evaluation took more time than the scheduled
          interval. I indicates a slower storage backend access or too complex
          query.'
      expr: 'prometheus_rule_group_last_duration_seconds > prometheus_rule_group_interval_seconds'
      labels:
        severity: warning
    - alert: Prometheus notifications backlog
      annotations:
        message: The Prometheus notification queue has not been empty for 10 minutes
      expr: 'min_over_time(prometheus_notifications_queue_length[10m]) > 0'
      labels:
        severity: warning
    - alert: Prometheus AlertManager notification failing
      annotations:
        message: Alertmanager is failing sending notifications
      expr: 'rate(alertmanager_notifications_failed_total[1m]) > 0'
      labels:
        severity: critical
    - alert: Prometheus target empty
      annotations:
        message: Prometheus has no target in service discovery
      expr: 'prometheus_sd_discovered_targets == 0'
      labels:
        severity: critical
    - alert: Prometheus target scraping slow
      annotations:
        message: Prometheus is scraping exporters slowly
      expr: 'prometheus_target_interval_length_seconds{quantile="0.9"} > 60'
      labels:
        severity: warning
    - alert: Prometheus large scrape
      annotations:
        message: Prometheus has many scrapes that exceed the sample limit
      expr: 'increase(prometheus_target_scrapes_exceeded_sample_limit_total[10m])
        > 10'
      labels:
        severity: warning
    - alert: Prometheus target scrape duplicate
      annotations:
        message: Prometheus has many samples rejected due to duplicate timestamps
          but different values
      expr: 'increase(prometheus_target_scrapes_sample_duplicate_timestamp_total[5m])
        > 0'
      labels:
        severity: warning
    - alert: Prometheus TSDB checkpoint creation failures
      annotations:
        message: 'Prometheus encountered {{ $value }} checkpoint creation failures'
      expr: 'increase(prometheus_tsdb_checkpoint_creations_failed_total[3m]) >
        0'
      labels:
        severity: critical
    - alert: Prometheus TSDB checkpoint deletion failures
      annotations:
        message: 'Prometheus encountered {{ $value }} checkpoint deletion failures'
      expr: 'increase(prometheus_tsdb_checkpoint_deletions_failed_total[3m]) >
        0'
      labels:
        severity: critical
    - alert: Prometheus TSDB compactions failed
      annotations:
        message: 'Prometheus encountered {{ $value }} TSDB compactions failures'
      expr: 'increase(prometheus_tsdb_compactions_failed_total[3m]) > 0'
      labels:
        severity: critical
    - alert: Prometheus TSDB head truncations failed
      annotations:
        message: 'Prometheus encountered {{ $value }} TSDB head truncation failures'
      expr: 'increase(prometheus_tsdb_head_truncations_failed_total[3m]) > 0'
      labels:
        severity: critical
    - alert: Prometheus TSDB reload failures
      annotations:
        message: 'Prometheus encountered {{ $value }} TSDB reload failures'
      expr: 'increase(prometheus_tsdb_reloads_failures_total[3m]) > 0'
      labels:
        severity: critical
    - alert: Prometheus TSDB WAL corruptions
      annotations:
        message: 'Prometheus encountered {{ $value }} TSDB WAL corruptions'
      expr: 'increase(prometheus_tsdb_wal_corruptions_total[3m]) > 0'
      labels:
        severity: critical
    - alert: Prometheus TSDB WAL truncations failed
      annotations:
        message: 'Prometheus encountered {{ $value }} TSDB WAL truncation failures'
      expr: 'increase(prometheus_tsdb_wal_truncations_failed_total[3m]) > 0'
      labels:
        severity: critical